{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP Augmentation Hands-On\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Augmentation in Computer Vision is one of the important techniques and has proved to be effective. \n",
    "In NLP, augmentation is also tried and shown imporovements in quite a few cases.\n",
    "\n",
    "In this part, We will first undestand the following\n",
    "\n",
    "    -What Data Augmentation is and why it works?\n",
    "\n",
    "    -Why it works so well Computer Vision?\n",
    "\n",
    "    -Benefits on Augmentation.\n",
    "    \n",
    "    -Types of NLP Augmentation.\n",
    "\n",
    "Then we will jump into one of the types of NLP Augmentation and will do hands-on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What augmentation is and why it works?\n",
    "\n",
    "Data Augmentation is a technique to sythetically generate new data points such that generated data have same semantics\n",
    "as of original data. In other words Data Augmentation is semantically invariant transformation.\n",
    "\n",
    "Data Augmentation has these primary reasons to work.\n",
    "\n",
    "- Data Scarcity\n",
    "\n",
    "\n",
    "- Improves generalization capabilities (reuce overfitting)\n",
    "\n",
    "\n",
    "- Test Time Augmentation (Confident Prediction)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why it works so well Computer Vision?\n",
    "\n",
    "In Computer vision, particulally Deep Learning algorithms are data hungary. It means more data is always welcome.\n",
    "\n",
    "Though there are some researcher object the volume vs quality of data. If you want to undestand more aabout it please\n",
    "go through this https://www.slideshare.net/xamat/10-lessons-learned-from-building-machine-learning-systems\n",
    "\n",
    "\n",
    "Transformations applied on image during augmenation still preserve the meaning, hence are semantically invariant transformation. (reference - https://medium.com/secure-and-private-ai-writing-challenge/data-augmentation-increases-accuracy-of-your-model-but-how-aa1913468722)\n",
    "\n",
    "![\"Image Aumentation\"](image_aug_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rules of Data Augmentation \n",
    "\n",
    "1. The augmented data must follow a statistical distribution similar to that of the original data.\n",
    "\n",
    "\n",
    "2. A human being should not be able to distinguish between the amplified data and the original data.\n",
    "\n",
    "\n",
    "3. Data augmentation involves semantically invariant transformations.\n",
    "\n",
    "\n",
    "4. In supervised learning, the transformations allowed for data augmentation are those that do not modify the class label of the new data generated.\n",
    "\n",
    "\n",
    "5. In order to respect the semantic invariance, the number of successive or combined transformations must be limited, empirically to two (2).\n",
    "\n",
    "\n",
    "Reference for above Rules [Text Data Augmentation Made Simple](https://arxiv.org/abs/1812.04718)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Benefits of Data Augmentation\n",
    "\n",
    "Benefits of augmentation is widely docoments in Computer vision research.\n",
    "\n",
    "- Implicit regularization\n",
    "\n",
    "\n",
    "- Semi-Supervised applications, insufficient data.\n",
    "\n",
    "\n",
    "- Cost effective way to data gathering and labeling. Automated synthetic data generation helps to alliviate tedious data collection process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have some understanding of Data Augmentation we will shift our attention to text augmentation. Text augmentation and NLP Augmentation could be treated as synonym.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NLP augmentation** can be classified into these major categories. Which each category having bunch of techniques.\n",
    "\n",
    "\n",
    "#### Categories of NLP Augmentation\n",
    "\n",
    "- Lexical Substitution\n",
    "\n",
    "\n",
    "- Back Translation\n",
    "\n",
    "\n",
    "- Text Surface Transformation\n",
    "\n",
    "\n",
    "- Random Noise Injection\n",
    "\n",
    "\n",
    "- Instance Crossover Augmentation\n",
    "\n",
    "\n",
    "- Syntax-tree Manipulation\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we do hands-on for Lexical Substitution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism', 'comp.graphics', 'sci.med', 'soc.religion.christian']"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import unidecode\n",
    "categories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']\n",
    "twenty_train = fetch_20newsgroups(subset='train',\n",
    "                                  categories=categories,\n",
    "                                  shuffle=True,\n",
    "                                  remove=('headers', 'footers', 'quotes'),\n",
    "                                  random_state=42)\n",
    "twenty_train.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = twenty_train.data\n",
    "target = twenty_train.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(text):\n",
    "    soup = BeautifulSoup(text)\n",
    "    html_pattern = re.compile('<.*?>')\n",
    "    text = html_pattern.sub(r' ', soup.text)\n",
    "    text = unidecode.unidecode(text)\n",
    "    text = re.sub('[^A-Za-z0-9.]+', ' ', text)\n",
    "    text = text.lower()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [clean_data(txt) for txt in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2257, 28179)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(data)\n",
    "X_train_counts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "mnb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.84\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean Accuracy: {:.2}\".format(cross_val_score(mnb, X_train_counts, target, cv=5).mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'sci.med', 'soc.religion.christian']\n",
      "(2257, 28179)\n",
      "Mean Accuracy: 0.84\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we will experiment with Lexical Substitution using NLTK wordnet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tag import pos_tag\n",
    "from nltk import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_synonym_for_word(word):\n",
    "    \"\"\"returns the synonym given word if found, otherwise returns the same word\"\"\"\n",
    "    synonyms = []\n",
    "    for syn in wordnet.synsets(word):\n",
    "\n",
    "        for l in syn.lemmas():\n",
    "            synonyms.append(l.name())\n",
    "    synonyms = [syn for syn in synonyms if syn!=word]\n",
    "    if len(synonyms) == 0:\n",
    "        return word\n",
    "    else:\n",
    "        return synonyms[0]\n",
    "\n",
    "def augment_sentence_wordnet(sentence, filters=['NN', 'JJ']):\n",
    "    \"\"\"Augments words in sentence which are filtered by pos tags\"\"\"\n",
    "    \n",
    "    pos_sent = pos_tag(sentence.split())\n",
    "    new_sent = []\n",
    "    for word,tag in pos_sent:\n",
    "        if tag in filters:\n",
    "            new_sent.append(get_synonym_for_word(word))\n",
    "        else:\n",
    "            new_sent.append(word)\n",
    "            \n",
    "    return \" \".join(new_sent)\n",
    "\n",
    "def augment_data(data, target):\n",
    "    \"\"\"Creates augmented data using wordnet synonym imputation.\"\"\"\n",
    "    \n",
    "    aug_data = []\n",
    "    aug_target = []\n",
    "    for row, t in zip(data, target):\n",
    "        aug_row = []\n",
    "        row_sents = sent_tokenize(row)\n",
    "        #print(\"row_sents\", row_sents)\n",
    "        for line in row_sents:\n",
    "            line = augment_sentence_wordnet(line)\n",
    "            aug_row.append(line)\n",
    "        row_sents = \" \".join(aug_row)\n",
    "        \n",
    "        #print(row_sents)\n",
    "        aug_data.append(row)\n",
    "        aug_data.append(row_sents)\n",
    "        aug_target.append(t)\n",
    "        aug_target.append(t)\n",
    "        #print(len(aug_data))\n",
    "    return aug_data, aug_target\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "aug_data, aug_target = augment_data(data, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "X_train_aug_counts = count_vect.fit_transform(aug_data)\n",
    "\n",
    "\n",
    "mnb_aug = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Accuracy: 0.85\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean Accuracy: {:.2}\".format(cross_val_score(mnb_aug, X_train_aug_counts, aug_target, cv=5).mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'sci.med', 'soc.religion.christian']\n",
      "(4514, 32279)\n",
      "Mean Accuracy: 0.85\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import unidecode\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.tag import pos_tag\n",
    "from nltk import sent_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_synonym_for_word(word):\n",
    "    \"\"\"returns the synonym given word if found, otherwise returns the same word\"\"\"\n",
    "    \n",
    "    synonyms = []\n",
    "    for syn in wordnet.synsets(word):\n",
    "\n",
    "        for l in syn.lemmas():\n",
    "            synonyms.append(l.name())\n",
    "    synonyms = [syn for syn in synonyms if syn!=word]\n",
    "    if len(synonyms) == 0:\n",
    "        return word\n",
    "    else:\n",
    "        return synonyms[0]\n",
    "\n",
    "def augment_sentence_wordnet(sentence, filters=['NN', 'JJ']):\n",
    "    \"\"\"Augments words in sentence which are filtered by pos tags\"\"\"\n",
    "    \n",
    "    pos_sent = pos_tag(sentence.split())\n",
    "    new_sent = []\n",
    "    for word,tag in pos_sent:\n",
    "        if tag in filters:\n",
    "            new_sent.append(get_synonym_for_word(word))\n",
    "        else:\n",
    "            new_sent.append(word)\n",
    "            \n",
    "    return \" \".join(new_sent)\n",
    "\n",
    "def augment_data(data, target):\n",
    "    \"\"\"Creates augmented data using wordnet synonym imputation.\"\"\"\n",
    "    \n",
    "    aug_data = []\n",
    "    aug_target = []\n",
    "    for row, t in zip(data, target):\n",
    "        aug_row = []\n",
    "        row_sents = sent_tokenize(row)\n",
    "        #print(\"row_sents\", row_sents)\n",
    "        for line in row_sents:\n",
    "            line = augment_sentence_wordnet(line)\n",
    "            aug_row.append(line)\n",
    "        row_sents = \" \".join(aug_row)\n",
    "        \n",
    "        #print(row_sents)\n",
    "        aug_data.append(row)\n",
    "        aug_data.append(row_sents)\n",
    "        aug_target.append(t)\n",
    "        aug_target.append(t)\n",
    "        #print(len(aug_data))\n",
    "    return aug_data, aug_target\n",
    "\n",
    "def clean_data(text):\n",
    "    soup = BeautifulSoup(text)\n",
    "    html_pattern = re.compile('<.*?>')\n",
    "    text = html_pattern.sub(r' ', soup.text)\n",
    "    text = unidecode.unidecode(text)\n",
    "    text = re.sub('[^A-Za-z0-9.]+', ' ', text)\n",
    "    text = text.lower()\n",
    "    \n",
    "    return text\n",
    "    \n",
    "categories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']\n",
    "twenty_train = fetch_20newsgroups(subset='train',\n",
    "                                  categories=categories,\n",
    "                                  shuffle=True,\n",
    "                                  remove=('headers', 'footers', 'quotes'),\n",
    "                                  random_state=42)\n",
    "print(twenty_train.target_names)\n",
    "\n",
    "data = twenty_train.data\n",
    "data = [clean_data(txt) for txt in data]\n",
    "target = twenty_train.target\n",
    "\n",
    "aug_data, aug_target = augment_data(data, target)\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "X_train_aug_counts = count_vect.fit_transform(aug_data)\n",
    "print(X_train_aug_counts.shape)\n",
    "\n",
    "mnb_aug = MultinomialNB()\n",
    "print(\"Mean Accuracy: {:.2}\".format(cross_val_score(mnb_aug, X_train_aug_counts, aug_target, cv=5).mean()))\n",
    "# We get 85% accuracy here, during my experiment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
